'''
Processes `TableShells` xls files from the Census FTP site, merging that data
with information from the output of `process_sequences.py` to produce a new csv
with complete metadata for every column of every table.

To run, first download the proper Census release sequence file, and run
`process_sequences.py` against it. Next, fetch a `TableShells.xls` file from
the same Census release, e.g. ftp://ftp2.census.gov/acs2011_1yr/summaryfile/

Then run this script with two arguments.

- The -f flag gets the filepath to the `TableShells` file you downloaded.
- The -m flag gets the filepath to the csv generated by `process_sequences`.
This csv will contain subject_area and other data to merge here, based on
the `Table ID` value.

>> python process_shells.py -f shell_files/file.xls -m sequence_files/file.xls

This will write a `Metadata` csv into `OUTPUT_DIR`, with columns for each
field in `FIELDS_FOR_CSV`.
'''
import csv, optparse, os, sys, traceback
from os.path import isdir, join, normpath
from xlrd import open_workbook, colname

from __init__ import OUTPUT_DIR

FIELDS_FOR_CSV = [
    'source',
    'table_id',
    'table_name',
    'table_universe',
    'table_size',
    'column_id',
    'column_order',
    'column_name',
    'indent',
    'parent',
    'has_children',
    'subject_area'
]

def get_subject_area_dict(filename):
    subject_area_dict = {}
    with open(filename, 'rb') as csvfile:
        reader = csv.DictReader(
            csvfile,
            restkey='Table ID'
        )
        for row in reader:
            subject_area_dict[row['table_id']] = row

    return subject_area_dict

def process_xls_file(filename, subject_area_dict):
    output_rows = []
    xlsfile = open_workbook(filename, formatting_info=True)
    sheet = xlsfile.sheet_by_index(0)
    num_rows = sheet.nrows
    keys = sheet.row_values(0)
    keys = [key.replace('\n',' ').strip() for key in keys]
    parent_of_dict = {}

    for row_index in range(1, num_rows):
        values = [unicode(cell.value).strip() for cell in sheet.row(row_index)]
        row = dict(zip(keys, values))
        print row_index
        if row['Table ID'] and not row['Stub'].startswith('Universe'):
            # Standard values for all tables and fields. This adds some data
            # from the `Table_Names_and_Subject_Areas` csv previously generated
            # by `process_sequences.py`. So if there's a `Table ID` mismatch
            # between the sequence and shell files, we'll throw an error here.
            # (This actually exists in the ACS 2009 5-Year release.)
            # TODO: suppress error, but collect list of missing `Table ID`s.
            stored_row = {
                'source': subject_area_dict[row['Table ID']]['source'],
                'table_id': row['Table ID'],
                'table_name': subject_area_dict[row['Table ID']]['table_name'],
                'table_universe': None,
                'table_size': subject_area_dict[row['Table ID']]['table_size'],
                'column_id': None,
                'column_order': None,
                'column_name': None,
                'indent': None,
                'parent': None,
                'has_children': False,
                'subject_area': subject_area_dict[row['Table ID']]['subject_area'],
            }
            if not row['Unique ID']:
                # we have a table name, and the universe is in the next row
                current_universe = sheet.row(row_index+1)[3].value
                current_universe = current_universe.replace('Universe:','').strip()
                stored_row.update({
                    'table_universe': current_universe
                })
            elif row['Unique ID']:
                # we have a field, so figure out parents and children
                cell = sheet.cell(row_index, 3)
                indent = xlsfile.xf_list[cell.xf_index].alignment.indent_level
                
                try:
                    next_cell = sheet.cell(row_index+1, 3)
                    next_indent = xlsfile.xf_list[next_cell.xf_index].alignment.indent_level
                    if next_indent > indent:
                        # this is a parent
                        stored_row.update({
                            'has_children': True,
                        })
                        parent_of_dict[str(next_indent)] = row['Unique ID']
                except IndexError:
                    # last row
                    pass

                if indent > 0:
                    # this is a child
                    parent_field = parent_of_dict[str(indent)]
                    stored_row.update({
                        'parent': parent_field,
                    })
                
                # handle inconsistent column names in shell files
                # will throw an error if we run into something new
                try:
                    column_order = row['Order']
                except:
                    column_order = row['Line']
                    
                stored_row.update({
                    'table_universe': current_universe,
                    'column_id': row['Unique ID'],
                    'column_order': column_order,
                    'column_name': unicode(row['Stub']).encode('utf-8'),
                    'indent': indent,
                })
            
            output_rows.append(stored_row)

    xlsfile.unload_sheet(0)
    write_csv(filename, output_rows)

def write_csv(filename, dict_list):
    csvfilename = os.path.basename(filename)
    csvfilename = csvfilename.replace(
        'Shells',
        '_Metadata'
    ).replace('.xls','.csv')
    csvpath = normpath(join(OUTPUT_DIR, csvfilename))

    print "Writing: " + csvpath + " ...\n"

    with open(csvpath,'wb') as csvfile:
        csvwriter = csv.DictWriter(
            csvfile,
            FIELDS_FOR_CSV,
            extrasaction='ignore',
            quoting=csv.QUOTE_ALL
        )
        csvwriter.writeheader()
        for item in dict_list:
            csvwriter.writerow(item)

def process_options(arglist=None):
    global options, args
    parser = optparse.OptionParser()
    parser.add_option(
        '-f', '--filename',
        dest='filename',
        help='filename of shell file to process',
    )
    parser.add_option(
        '-m', '--merge',
        dest='merge',
        help='filename of sequence csv with table IDs and subject areas',
    )
    options, args = parser.parse_args(arglist)
    return options, args


def main(args=None):
    """
    >> python process_shells.py -f shell_files/file.xls -m sequence_files/file.xls
    """
    if args is None:
        args = sys.argv[1:]
    options, args = process_options(args)

    # make sure we have the expected directories
    for path in [OUTPUT_DIR,]:
        if not isdir(path):
            os.mkdir(path)

    subject_area_dict = get_subject_area_dict(options.merge)
    process_xls_file(options.filename, subject_area_dict)

if __name__ == '__main__':
    try:
        main()
    except Exception, e:
        sys.stderr.write('\n')
        traceback.print_exc(file=sys.stderr)
        sys.stderr.write('\n')
        sys.exit(1)